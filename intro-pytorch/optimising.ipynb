{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base model and data that we're going to train\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set hyperparameters first (parameters that arent trained)\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we're going to train this model in an optimisation loop. each iteration of the loop is called an epoch\n",
    "\n",
    "every epoch got two parts:\n",
    "- train loop - iterate over the train dataset and attempt to converge to optimal parameters\n",
    "- validation/test loop - iterate over test dataset and see if performance is improving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train loop:\n",
    "\n",
    "the loss function is very important here. it measures the level of dissimilarity of the predicted to the actual result. \n",
    "to calculate loss, we make a prediction and compare it to the true data label value (ground truth)\n",
    "\n",
    "Common loss functions include nn.MSELoss (Mean Square Error) for regression tasks, and nn.NLLLoss (Negative Log Likelihood) for classification. nn.CrossEntropyLoss combines nn.LogSoftmax and nn.NLLLoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss() # normalize the logits and compute the prediction error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimisation is adjusting model params to reduce the model error. optimisation algos determine how this is performed. \n",
    "\n",
    "for this example, we use stochastic gradient descent. all optimisation logic is encapsulated and abstracted within the optimizer object. \n",
    "\n",
    "other optimisers include ADAM or RMSProp, which work better for different models and data. \n",
    "\n",
    "We initialize the optimizer by registering the modelâ€™s parameters that need to be trained, and passing in the learning rate hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the training loop, optimization happens in three steps:\n",
    "1. Call optimizer.zero_grad() to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
    "\n",
    "2. Backpropagate the prediction loss with a call to loss.backward(). PyTorch deposits the gradients of the loss w.r.t. each parameter.\n",
    "\n",
    "3. Once we have our gradients, we call optimizer.step() to adjust the parameters by the gradients collected in the backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>full implementation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward() # compute the gradient of the loss with respect to the model's parameters\n",
    "        optimizer.step() # adjust the model's parameters based on the computed gradients\n",
    "        optimizer.zero_grad() # reset the gradients to zero\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.294316  [   64/60000]\n",
      "loss: 2.289581  [ 6464/60000]\n",
      "loss: 2.272528  [12864/60000]\n",
      "loss: 2.278118  [19264/60000]\n",
      "loss: 2.251801  [25664/60000]\n",
      "loss: 2.228192  [32064/60000]\n",
      "loss: 2.231961  [38464/60000]\n",
      "loss: 2.202521  [44864/60000]\n",
      "loss: 2.202631  [51264/60000]\n",
      "loss: 2.174229  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 52.0%, Avg loss: 2.164175 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.169091  [   64/60000]\n",
      "loss: 2.161283  [ 6464/60000]\n",
      "loss: 2.104441  [12864/60000]\n",
      "loss: 2.128984  [19264/60000]\n",
      "loss: 2.077442  [25664/60000]\n",
      "loss: 2.015485  [32064/60000]\n",
      "loss: 2.047304  [38464/60000]\n",
      "loss: 1.972486  [44864/60000]\n",
      "loss: 1.980292  [51264/60000]\n",
      "loss: 1.909984  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 1.901451 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.934215  [   64/60000]\n",
      "loss: 1.901701  [ 6464/60000]\n",
      "loss: 1.782995  [12864/60000]\n",
      "loss: 1.830464  [19264/60000]\n",
      "loss: 1.725582  [25664/60000]\n",
      "loss: 1.664601  [32064/60000]\n",
      "loss: 1.699261  [38464/60000]\n",
      "loss: 1.599376  [44864/60000]\n",
      "loss: 1.618379  [51264/60000]\n",
      "loss: 1.521925  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: 1.528039 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.591246  [   64/60000]\n",
      "loss: 1.554098  [ 6464/60000]\n",
      "loss: 1.402808  [12864/60000]\n",
      "loss: 1.484068  [19264/60000]\n",
      "loss: 1.365798  [25664/60000]\n",
      "loss: 1.347016  [32064/60000]\n",
      "loss: 1.374976  [38464/60000]\n",
      "loss: 1.298568  [44864/60000]\n",
      "loss: 1.323584  [51264/60000]\n",
      "loss: 1.237959  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 1.252133 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.322217  [   64/60000]\n",
      "loss: 1.304839  [ 6464/60000]\n",
      "loss: 1.141043  [12864/60000]\n",
      "loss: 1.256938  [19264/60000]\n",
      "loss: 1.128265  [25664/60000]\n",
      "loss: 1.139713  [32064/60000]\n",
      "loss: 1.173629  [38464/60000]\n",
      "loss: 1.111128  [44864/60000]\n",
      "loss: 1.140531  [51264/60000]\n",
      "loss: 1.070952  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 1.081856 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.145774  [   64/60000]\n",
      "loss: 1.148628  [ 6464/60000]\n",
      "loss: 0.969873  [12864/60000]\n",
      "loss: 1.116342  [19264/60000]\n",
      "loss: 0.983800  [25664/60000]\n",
      "loss: 1.002855  [32064/60000]\n",
      "loss: 1.050095  [38464/60000]\n",
      "loss: 0.992292  [44864/60000]\n",
      "loss: 1.022388  [51264/60000]\n",
      "loss: 0.967383  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.973627 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.025174  [   64/60000]\n",
      "loss: 1.049453  [ 6464/60000]\n",
      "loss: 0.853408  [12864/60000]\n",
      "loss: 1.024713  [19264/60000]\n",
      "loss: 0.894640  [25664/60000]\n",
      "loss: 0.908603  [32064/60000]\n",
      "loss: 0.970918  [38464/60000]\n",
      "loss: 0.916066  [44864/60000]\n",
      "loss: 0.942306  [51264/60000]\n",
      "loss: 0.900010  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.901674 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.938405  [   64/60000]\n",
      "loss: 0.982182  [ 6464/60000]\n",
      "loss: 0.772017  [12864/60000]\n",
      "loss: 0.961371  [19264/60000]\n",
      "loss: 0.836827  [25664/60000]\n",
      "loss: 0.841863  [32064/60000]\n",
      "loss: 0.916424  [38464/60000]\n",
      "loss: 0.865692  [44864/60000]\n",
      "loss: 0.885306  [51264/60000]\n",
      "loss: 0.852470  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.850830 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.872364  [   64/60000]\n",
      "loss: 0.932414  [ 6464/60000]\n",
      "loss: 0.711990  [12864/60000]\n",
      "loss: 0.915027  [19264/60000]\n",
      "loss: 0.796184  [25664/60000]\n",
      "loss: 0.792682  [32064/60000]\n",
      "loss: 0.875559  [38464/60000]\n",
      "loss: 0.830594  [44864/60000]\n",
      "loss: 0.842779  [51264/60000]\n",
      "loss: 0.816575  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.812685 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.819982  [   64/60000]\n",
      "loss: 0.892827  [ 6464/60000]\n",
      "loss: 0.665773  [12864/60000]\n",
      "loss: 0.879515  [19264/60000]\n",
      "loss: 0.765328  [25664/60000]\n",
      "loss: 0.755181  [32064/60000]\n",
      "loss: 0.842575  [38464/60000]\n",
      "loss: 0.804588  [44864/60000]\n",
      "loss: 0.809773  [51264/60000]\n",
      "loss: 0.787882  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.782371 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.776665  [   64/60000]\n",
      "loss: 0.859328  [ 6464/60000]\n",
      "loss: 0.628684  [12864/60000]\n",
      "loss: 0.851079  [19264/60000]\n",
      "loss: 0.740223  [25664/60000]\n",
      "loss: 0.725497  [32064/60000]\n",
      "loss: 0.814491  [38464/60000]\n",
      "loss: 0.784087  [44864/60000]\n",
      "loss: 0.782958  [51264/60000]\n",
      "loss: 0.763921  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.757042 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.739618  [   64/60000]\n",
      "loss: 0.829866  [ 6464/60000]\n",
      "loss: 0.597842  [12864/60000]\n",
      "loss: 0.827264  [19264/60000]\n",
      "loss: 0.718772  [25664/60000]\n",
      "loss: 0.701245  [32064/60000]\n",
      "loss: 0.789630  [38464/60000]\n",
      "loss: 0.766819  [44864/60000]\n",
      "loss: 0.760554  [51264/60000]\n",
      "loss: 0.743277  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.735097 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.707382  [   64/60000]\n",
      "loss: 0.803263  [ 6464/60000]\n",
      "loss: 0.571536  [12864/60000]\n",
      "loss: 0.806763  [19264/60000]\n",
      "loss: 0.699970  [25664/60000]\n",
      "loss: 0.681057  [32064/60000]\n",
      "loss: 0.767084  [38464/60000]\n",
      "loss: 0.751666  [44864/60000]\n",
      "loss: 0.741447  [51264/60000]\n",
      "loss: 0.725019  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.715585 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.678966  [   64/60000]\n",
      "loss: 0.779036  [ 6464/60000]\n",
      "loss: 0.548662  [12864/60000]\n",
      "loss: 0.788636  [19264/60000]\n",
      "loss: 0.683317  [25664/60000]\n",
      "loss: 0.663946  [32064/60000]\n",
      "loss: 0.746312  [38464/60000]\n",
      "loss: 0.738065  [44864/60000]\n",
      "loss: 0.724835  [51264/60000]\n",
      "loss: 0.708621  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.697948 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.653649  [   64/60000]\n",
      "loss: 0.756793  [ 6464/60000]\n",
      "loss: 0.528587  [12864/60000]\n",
      "loss: 0.772231  [19264/60000]\n",
      "loss: 0.668430  [25664/60000]\n",
      "loss: 0.649138  [32064/60000]\n",
      "loss: 0.726905  [38464/60000]\n",
      "loss: 0.725854  [44864/60000]\n",
      "loss: 0.710392  [51264/60000]\n",
      "loss: 0.693670  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.681844 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.631014  [   64/60000]\n",
      "loss: 0.736347  [ 6464/60000]\n",
      "loss: 0.510856  [12864/60000]\n",
      "loss: 0.757265  [19264/60000]\n",
      "loss: 0.654991  [25664/60000]\n",
      "loss: 0.636205  [32064/60000]\n",
      "loss: 0.708837  [38464/60000]\n",
      "loss: 0.714932  [44864/60000]\n",
      "loss: 0.697782  [51264/60000]\n",
      "loss: 0.679828  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.667056 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.610702  [   64/60000]\n",
      "loss: 0.717496  [ 6464/60000]\n",
      "loss: 0.495045  [12864/60000]\n",
      "loss: 0.743383  [19264/60000]\n",
      "loss: 0.642850  [25664/60000]\n",
      "loss: 0.624847  [32064/60000]\n",
      "loss: 0.692035  [38464/60000]\n",
      "loss: 0.705294  [44864/60000]\n",
      "loss: 0.686727  [51264/60000]\n",
      "loss: 0.667066  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.653450 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.592366  [   64/60000]\n",
      "loss: 0.700122  [ 6464/60000]\n",
      "loss: 0.480819  [12864/60000]\n",
      "loss: 0.730420  [19264/60000]\n",
      "loss: 0.631991  [25664/60000]\n",
      "loss: 0.614785  [32064/60000]\n",
      "loss: 0.676417  [38464/60000]\n",
      "loss: 0.696767  [44864/60000]\n",
      "loss: 0.677153  [51264/60000]\n",
      "loss: 0.655195  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.640916 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.575772  [   64/60000]\n",
      "loss: 0.684108  [ 6464/60000]\n",
      "loss: 0.468035  [12864/60000]\n",
      "loss: 0.718211  [19264/60000]\n",
      "loss: 0.622179  [25664/60000]\n",
      "loss: 0.605828  [32064/60000]\n",
      "loss: 0.661830  [38464/60000]\n",
      "loss: 0.689352  [44864/60000]\n",
      "loss: 0.668920  [51264/60000]\n",
      "loss: 0.644074  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.629355 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.560699  [   64/60000]\n",
      "loss: 0.669302  [ 6464/60000]\n",
      "loss: 0.456429  [12864/60000]\n",
      "loss: 0.706717  [19264/60000]\n",
      "loss: 0.613174  [25664/60000]\n",
      "loss: 0.597806  [32064/60000]\n",
      "loss: 0.648233  [38464/60000]\n",
      "loss: 0.683004  [44864/60000]\n",
      "loss: 0.661896  [51264/60000]\n",
      "loss: 0.633572  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.3%, Avg loss: 0.618681 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.546947  [   64/60000]\n",
      "loss: 0.655537  [ 6464/60000]\n",
      "loss: 0.445853  [12864/60000]\n",
      "loss: 0.695797  [19264/60000]\n",
      "loss: 0.604748  [25664/60000]\n",
      "loss: 0.590609  [32064/60000]\n",
      "loss: 0.635558  [38464/60000]\n",
      "loss: 0.677654  [44864/60000]\n",
      "loss: 0.655904  [51264/60000]\n",
      "loss: 0.623618  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.608820 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.534291  [   64/60000]\n",
      "loss: 0.642778  [ 6464/60000]\n",
      "loss: 0.436157  [12864/60000]\n",
      "loss: 0.685483  [19264/60000]\n",
      "loss: 0.596740  [25664/60000]\n",
      "loss: 0.584088  [32064/60000]\n",
      "loss: 0.623758  [38464/60000]\n",
      "loss: 0.673273  [44864/60000]\n",
      "loss: 0.650801  [51264/60000]\n",
      "loss: 0.614132  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.599693 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.522582  [   64/60000]\n",
      "loss: 0.630903  [ 6464/60000]\n",
      "loss: 0.427258  [12864/60000]\n",
      "loss: 0.675717  [19264/60000]\n",
      "loss: 0.589121  [25664/60000]\n",
      "loss: 0.578089  [32064/60000]\n",
      "loss: 0.612739  [38464/60000]\n",
      "loss: 0.669716  [44864/60000]\n",
      "loss: 0.646466  [51264/60000]\n",
      "loss: 0.605018  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.591246 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.511688  [   64/60000]\n",
      "loss: 0.619881  [ 6464/60000]\n",
      "loss: 0.419068  [12864/60000]\n",
      "loss: 0.666436  [19264/60000]\n",
      "loss: 0.581796  [25664/60000]\n",
      "loss: 0.572464  [32064/60000]\n",
      "loss: 0.602471  [38464/60000]\n",
      "loss: 0.666905  [44864/60000]\n",
      "loss: 0.642836  [51264/60000]\n",
      "loss: 0.596291  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 0.583423 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.501490  [   64/60000]\n",
      "loss: 0.609635  [ 6464/60000]\n",
      "loss: 0.411496  [12864/60000]\n",
      "loss: 0.657619  [19264/60000]\n",
      "loss: 0.574686  [25664/60000]\n",
      "loss: 0.567166  [32064/60000]\n",
      "loss: 0.592955  [38464/60000]\n",
      "loss: 0.664738  [44864/60000]\n",
      "loss: 0.639819  [51264/60000]\n",
      "loss: 0.587815  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.576168 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.491932  [   64/60000]\n",
      "loss: 0.600122  [ 6464/60000]\n",
      "loss: 0.404421  [12864/60000]\n",
      "loss: 0.649205  [19264/60000]\n",
      "loss: 0.567754  [25664/60000]\n",
      "loss: 0.562141  [32064/60000]\n",
      "loss: 0.584104  [38464/60000]\n",
      "loss: 0.663158  [44864/60000]\n",
      "loss: 0.637225  [51264/60000]\n",
      "loss: 0.579578  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.569439 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.482954  [   64/60000]\n",
      "loss: 0.591262  [ 6464/60000]\n",
      "loss: 0.397793  [12864/60000]\n",
      "loss: 0.641156  [19264/60000]\n",
      "loss: 0.560958  [25664/60000]\n",
      "loss: 0.557285  [32064/60000]\n",
      "loss: 0.575833  [38464/60000]\n",
      "loss: 0.662099  [44864/60000]\n",
      "loss: 0.634964  [51264/60000]\n",
      "loss: 0.571549  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.563189 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.474470  [   64/60000]\n",
      "loss: 0.583015  [ 6464/60000]\n",
      "loss: 0.391610  [12864/60000]\n",
      "loss: 0.633493  [19264/60000]\n",
      "loss: 0.554311  [25664/60000]\n",
      "loss: 0.552500  [32064/60000]\n",
      "loss: 0.568130  [38464/60000]\n",
      "loss: 0.661501  [44864/60000]\n",
      "loss: 0.632974  [51264/60000]\n",
      "loss: 0.563765  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.557372 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.466441  [   64/60000]\n",
      "loss: 0.575321  [ 6464/60000]\n",
      "loss: 0.385813  [12864/60000]\n",
      "loss: 0.626175  [19264/60000]\n",
      "loss: 0.547776  [25664/60000]\n",
      "loss: 0.547862  [32064/60000]\n",
      "loss: 0.560973  [38464/60000]\n",
      "loss: 0.661285  [44864/60000]\n",
      "loss: 0.631127  [51264/60000]\n",
      "loss: 0.556212  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.551953 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.458882  [   64/60000]\n",
      "loss: 0.568146  [ 6464/60000]\n",
      "loss: 0.380399  [12864/60000]\n",
      "loss: 0.619192  [19264/60000]\n",
      "loss: 0.541355  [25664/60000]\n",
      "loss: 0.543295  [32064/60000]\n",
      "loss: 0.554289  [38464/60000]\n",
      "loss: 0.661345  [44864/60000]\n",
      "loss: 0.629413  [51264/60000]\n",
      "loss: 0.548858  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.546905 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.451642  [   64/60000]\n",
      "loss: 0.561450  [ 6464/60000]\n",
      "loss: 0.375336  [12864/60000]\n",
      "loss: 0.612516  [19264/60000]\n",
      "loss: 0.535084  [25664/60000]\n",
      "loss: 0.538753  [32064/60000]\n",
      "loss: 0.548052  [38464/60000]\n",
      "loss: 0.661614  [44864/60000]\n",
      "loss: 0.627851  [51264/60000]\n",
      "loss: 0.541730  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.542189 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.444740  [   64/60000]\n",
      "loss: 0.555218  [ 6464/60000]\n",
      "loss: 0.370577  [12864/60000]\n",
      "loss: 0.606088  [19264/60000]\n",
      "loss: 0.528969  [25664/60000]\n",
      "loss: 0.534217  [32064/60000]\n",
      "loss: 0.542230  [38464/60000]\n",
      "loss: 0.662067  [44864/60000]\n",
      "loss: 0.626404  [51264/60000]\n",
      "loss: 0.534816  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.537776 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.438121  [   64/60000]\n",
      "loss: 0.549444  [ 6464/60000]\n",
      "loss: 0.366023  [12864/60000]\n",
      "loss: 0.599931  [19264/60000]\n",
      "loss: 0.522989  [25664/60000]\n",
      "loss: 0.529724  [32064/60000]\n",
      "loss: 0.536772  [38464/60000]\n",
      "loss: 0.662580  [44864/60000]\n",
      "loss: 0.624973  [51264/60000]\n",
      "loss: 0.528140  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.533647 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.431754  [   64/60000]\n",
      "loss: 0.544092  [ 6464/60000]\n",
      "loss: 0.361735  [12864/60000]\n",
      "loss: 0.594104  [19264/60000]\n",
      "loss: 0.517131  [25664/60000]\n",
      "loss: 0.525261  [32064/60000]\n",
      "loss: 0.531627  [38464/60000]\n",
      "loss: 0.663185  [44864/60000]\n",
      "loss: 0.623504  [51264/60000]\n",
      "loss: 0.521701  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.529771 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.425683  [   64/60000]\n",
      "loss: 0.539121  [ 6464/60000]\n",
      "loss: 0.357664  [12864/60000]\n",
      "loss: 0.588530  [19264/60000]\n",
      "loss: 0.511429  [25664/60000]\n",
      "loss: 0.520861  [32064/60000]\n",
      "loss: 0.526781  [38464/60000]\n",
      "loss: 0.663780  [44864/60000]\n",
      "loss: 0.622007  [51264/60000]\n",
      "loss: 0.515497  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.526129 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.419798  [   64/60000]\n",
      "loss: 0.534482  [ 6464/60000]\n",
      "loss: 0.353808  [12864/60000]\n",
      "loss: 0.583177  [19264/60000]\n",
      "loss: 0.505923  [25664/60000]\n",
      "loss: 0.516466  [32064/60000]\n",
      "loss: 0.522185  [38464/60000]\n",
      "loss: 0.664371  [44864/60000]\n",
      "loss: 0.620477  [51264/60000]\n",
      "loss: 0.509514  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.522695 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.414167  [   64/60000]\n",
      "loss: 0.530164  [ 6464/60000]\n",
      "loss: 0.350147  [12864/60000]\n",
      "loss: 0.577992  [19264/60000]\n",
      "loss: 0.500557  [25664/60000]\n",
      "loss: 0.512092  [32064/60000]\n",
      "loss: 0.517857  [38464/60000]\n",
      "loss: 0.664891  [44864/60000]\n",
      "loss: 0.618931  [51264/60000]\n",
      "loss: 0.503721  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.7%, Avg loss: 0.519452 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.408696  [   64/60000]\n",
      "loss: 0.526136  [ 6464/60000]\n",
      "loss: 0.346604  [12864/60000]\n",
      "loss: 0.573050  [19264/60000]\n",
      "loss: 0.495409  [25664/60000]\n",
      "loss: 0.507886  [32064/60000]\n",
      "loss: 0.513795  [38464/60000]\n",
      "loss: 0.665304  [44864/60000]\n",
      "loss: 0.617278  [51264/60000]\n",
      "loss: 0.498184  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.516385 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.403413  [   64/60000]\n",
      "loss: 0.522351  [ 6464/60000]\n",
      "loss: 0.343234  [12864/60000]\n",
      "loss: 0.568318  [19264/60000]\n",
      "loss: 0.490483  [25664/60000]\n",
      "loss: 0.503699  [32064/60000]\n",
      "loss: 0.509932  [38464/60000]\n",
      "loss: 0.665632  [44864/60000]\n",
      "loss: 0.615592  [51264/60000]\n",
      "loss: 0.492916  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.513475 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.398308  [   64/60000]\n",
      "loss: 0.518778  [ 6464/60000]\n",
      "loss: 0.339958  [12864/60000]\n",
      "loss: 0.563814  [19264/60000]\n",
      "loss: 0.485798  [25664/60000]\n",
      "loss: 0.499646  [32064/60000]\n",
      "loss: 0.506224  [38464/60000]\n",
      "loss: 0.665796  [44864/60000]\n",
      "loss: 0.613845  [51264/60000]\n",
      "loss: 0.487887  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.510719 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.393421  [   64/60000]\n",
      "loss: 0.515391  [ 6464/60000]\n",
      "loss: 0.336867  [12864/60000]\n",
      "loss: 0.559534  [19264/60000]\n",
      "loss: 0.481240  [25664/60000]\n",
      "loss: 0.495702  [32064/60000]\n",
      "loss: 0.502679  [38464/60000]\n",
      "loss: 0.665847  [44864/60000]\n",
      "loss: 0.612059  [51264/60000]\n",
      "loss: 0.483112  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.508101 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.388710  [   64/60000]\n",
      "loss: 0.512164  [ 6464/60000]\n",
      "loss: 0.333902  [12864/60000]\n",
      "loss: 0.555441  [19264/60000]\n",
      "loss: 0.476867  [25664/60000]\n",
      "loss: 0.491895  [32064/60000]\n",
      "loss: 0.499268  [38464/60000]\n",
      "loss: 0.665765  [44864/60000]\n",
      "loss: 0.610308  [51264/60000]\n",
      "loss: 0.478568  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.505612 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.384161  [   64/60000]\n",
      "loss: 0.509142  [ 6464/60000]\n",
      "loss: 0.331108  [12864/60000]\n",
      "loss: 0.551515  [19264/60000]\n",
      "loss: 0.472645  [25664/60000]\n",
      "loss: 0.488256  [32064/60000]\n",
      "loss: 0.495985  [38464/60000]\n",
      "loss: 0.665548  [44864/60000]\n",
      "loss: 0.608542  [51264/60000]\n",
      "loss: 0.474242  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.503237 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.379785  [   64/60000]\n",
      "loss: 0.506311  [ 6464/60000]\n",
      "loss: 0.328440  [12864/60000]\n",
      "loss: 0.547745  [19264/60000]\n",
      "loss: 0.468571  [25664/60000]\n",
      "loss: 0.484761  [32064/60000]\n",
      "loss: 0.492840  [38464/60000]\n",
      "loss: 0.665163  [44864/60000]\n",
      "loss: 0.606781  [51264/60000]\n",
      "loss: 0.470127  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.500965 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.375515  [   64/60000]\n",
      "loss: 0.503644  [ 6464/60000]\n",
      "loss: 0.325872  [12864/60000]\n",
      "loss: 0.544121  [19264/60000]\n",
      "loss: 0.464668  [25664/60000]\n",
      "loss: 0.481378  [32064/60000]\n",
      "loss: 0.489819  [38464/60000]\n",
      "loss: 0.664637  [44864/60000]\n",
      "loss: 0.604955  [51264/60000]\n",
      "loss: 0.466200  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.498791 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.371353  [   64/60000]\n",
      "loss: 0.501118  [ 6464/60000]\n",
      "loss: 0.323416  [12864/60000]\n",
      "loss: 0.540640  [19264/60000]\n",
      "loss: 0.460911  [25664/60000]\n",
      "loss: 0.478095  [32064/60000]\n",
      "loss: 0.486904  [38464/60000]\n",
      "loss: 0.663959  [44864/60000]\n",
      "loss: 0.603135  [51264/60000]\n",
      "loss: 0.462520  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.496706 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.367305  [   64/60000]\n",
      "loss: 0.498679  [ 6464/60000]\n",
      "loss: 0.321058  [12864/60000]\n",
      "loss: 0.537315  [19264/60000]\n",
      "loss: 0.457312  [25664/60000]\n",
      "loss: 0.474918  [32064/60000]\n",
      "loss: 0.484098  [38464/60000]\n",
      "loss: 0.663133  [44864/60000]\n",
      "loss: 0.601348  [51264/60000]\n",
      "loss: 0.459013  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.494707 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.363405  [   64/60000]\n",
      "loss: 0.496365  [ 6464/60000]\n",
      "loss: 0.318761  [12864/60000]\n",
      "loss: 0.534170  [19264/60000]\n",
      "loss: 0.453850  [25664/60000]\n",
      "loss: 0.471800  [32064/60000]\n",
      "loss: 0.481431  [38464/60000]\n",
      "loss: 0.662163  [44864/60000]\n",
      "loss: 0.599592  [51264/60000]\n",
      "loss: 0.455680  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.492785 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.359625  [   64/60000]\n",
      "loss: 0.494159  [ 6464/60000]\n",
      "loss: 0.316553  [12864/60000]\n",
      "loss: 0.531175  [19264/60000]\n",
      "loss: 0.450546  [25664/60000]\n",
      "loss: 0.468856  [32064/60000]\n",
      "loss: 0.478857  [38464/60000]\n",
      "loss: 0.661137  [44864/60000]\n",
      "loss: 0.597764  [51264/60000]\n",
      "loss: 0.452562  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.490933 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.355926  [   64/60000]\n",
      "loss: 0.492070  [ 6464/60000]\n",
      "loss: 0.314418  [12864/60000]\n",
      "loss: 0.528320  [19264/60000]\n",
      "loss: 0.447256  [25664/60000]\n",
      "loss: 0.466031  [32064/60000]\n",
      "loss: 0.476355  [38464/60000]\n",
      "loss: 0.660003  [44864/60000]\n",
      "loss: 0.595945  [51264/60000]\n",
      "loss: 0.449637  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.489145 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.352314  [   64/60000]\n",
      "loss: 0.490044  [ 6464/60000]\n",
      "loss: 0.312373  [12864/60000]\n",
      "loss: 0.525578  [19264/60000]\n",
      "loss: 0.444125  [25664/60000]\n",
      "loss: 0.463338  [32064/60000]\n",
      "loss: 0.473955  [38464/60000]\n",
      "loss: 0.658783  [44864/60000]\n",
      "loss: 0.594147  [51264/60000]\n",
      "loss: 0.446842  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.487423 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.348795  [   64/60000]\n",
      "loss: 0.488081  [ 6464/60000]\n",
      "loss: 0.310382  [12864/60000]\n",
      "loss: 0.522956  [19264/60000]\n",
      "loss: 0.441129  [25664/60000]\n",
      "loss: 0.460742  [32064/60000]\n",
      "loss: 0.471630  [38464/60000]\n",
      "loss: 0.657508  [44864/60000]\n",
      "loss: 0.592332  [51264/60000]\n",
      "loss: 0.444163  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.485754 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.345394  [   64/60000]\n",
      "loss: 0.486139  [ 6464/60000]\n",
      "loss: 0.308448  [12864/60000]\n",
      "loss: 0.520433  [19264/60000]\n",
      "loss: 0.438218  [25664/60000]\n",
      "loss: 0.458291  [32064/60000]\n",
      "loss: 0.469368  [38464/60000]\n",
      "loss: 0.656185  [44864/60000]\n",
      "loss: 0.590559  [51264/60000]\n",
      "loss: 0.441628  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.484139 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.342095  [   64/60000]\n",
      "loss: 0.484287  [ 6464/60000]\n",
      "loss: 0.306584  [12864/60000]\n",
      "loss: 0.518014  [19264/60000]\n",
      "loss: 0.435440  [25664/60000]\n",
      "loss: 0.455977  [32064/60000]\n",
      "loss: 0.467150  [38464/60000]\n",
      "loss: 0.654789  [44864/60000]\n",
      "loss: 0.588778  [51264/60000]\n",
      "loss: 0.439243  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.482574 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.338892  [   64/60000]\n",
      "loss: 0.482489  [ 6464/60000]\n",
      "loss: 0.304807  [12864/60000]\n",
      "loss: 0.515681  [19264/60000]\n",
      "loss: 0.432734  [25664/60000]\n",
      "loss: 0.453751  [32064/60000]\n",
      "loss: 0.465024  [38464/60000]\n",
      "loss: 0.653310  [44864/60000]\n",
      "loss: 0.586973  [51264/60000]\n",
      "loss: 0.436924  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.481056 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.335789  [   64/60000]\n",
      "loss: 0.480713  [ 6464/60000]\n",
      "loss: 0.303056  [12864/60000]\n",
      "loss: 0.513398  [19264/60000]\n",
      "loss: 0.430125  [25664/60000]\n",
      "loss: 0.451606  [32064/60000]\n",
      "loss: 0.462939  [38464/60000]\n",
      "loss: 0.651777  [44864/60000]\n",
      "loss: 0.585135  [51264/60000]\n",
      "loss: 0.434733  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.479581 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.332741  [   64/60000]\n",
      "loss: 0.478966  [ 6464/60000]\n",
      "loss: 0.301367  [12864/60000]\n",
      "loss: 0.511228  [19264/60000]\n",
      "loss: 0.427687  [25664/60000]\n",
      "loss: 0.449540  [32064/60000]\n",
      "loss: 0.460939  [38464/60000]\n",
      "loss: 0.650261  [44864/60000]\n",
      "loss: 0.583304  [51264/60000]\n",
      "loss: 0.432679  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.478143 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.329802  [   64/60000]\n",
      "loss: 0.477274  [ 6464/60000]\n",
      "loss: 0.299734  [12864/60000]\n",
      "loss: 0.509142  [19264/60000]\n",
      "loss: 0.425289  [25664/60000]\n",
      "loss: 0.447467  [32064/60000]\n",
      "loss: 0.458943  [38464/60000]\n",
      "loss: 0.648632  [44864/60000]\n",
      "loss: 0.581554  [51264/60000]\n",
      "loss: 0.430696  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.476746 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.326920  [   64/60000]\n",
      "loss: 0.475650  [ 6464/60000]\n",
      "loss: 0.298163  [12864/60000]\n",
      "loss: 0.507108  [19264/60000]\n",
      "loss: 0.423024  [25664/60000]\n",
      "loss: 0.445500  [32064/60000]\n",
      "loss: 0.457036  [38464/60000]\n",
      "loss: 0.647007  [44864/60000]\n",
      "loss: 0.579789  [51264/60000]\n",
      "loss: 0.428796  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.475378 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.324160  [   64/60000]\n",
      "loss: 0.474062  [ 6464/60000]\n",
      "loss: 0.296611  [12864/60000]\n",
      "loss: 0.505190  [19264/60000]\n",
      "loss: 0.420756  [25664/60000]\n",
      "loss: 0.443603  [32064/60000]\n",
      "loss: 0.455156  [38464/60000]\n",
      "loss: 0.645351  [44864/60000]\n",
      "loss: 0.578109  [51264/60000]\n",
      "loss: 0.426969  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.474049 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.321467  [   64/60000]\n",
      "loss: 0.472476  [ 6464/60000]\n",
      "loss: 0.295128  [12864/60000]\n",
      "loss: 0.503378  [19264/60000]\n",
      "loss: 0.418536  [25664/60000]\n",
      "loss: 0.441775  [32064/60000]\n",
      "loss: 0.453302  [38464/60000]\n",
      "loss: 0.643657  [44864/60000]\n",
      "loss: 0.576448  [51264/60000]\n",
      "loss: 0.425263  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.472755 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.318876  [   64/60000]\n",
      "loss: 0.470953  [ 6464/60000]\n",
      "loss: 0.293713  [12864/60000]\n",
      "loss: 0.501641  [19264/60000]\n",
      "loss: 0.416363  [25664/60000]\n",
      "loss: 0.439987  [32064/60000]\n",
      "loss: 0.451508  [38464/60000]\n",
      "loss: 0.642003  [44864/60000]\n",
      "loss: 0.574836  [51264/60000]\n",
      "loss: 0.423613  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.471495 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.316387  [   64/60000]\n",
      "loss: 0.469447  [ 6464/60000]\n",
      "loss: 0.292304  [12864/60000]\n",
      "loss: 0.499948  [19264/60000]\n",
      "loss: 0.414227  [25664/60000]\n",
      "loss: 0.438241  [32064/60000]\n",
      "loss: 0.449799  [38464/60000]\n",
      "loss: 0.640364  [44864/60000]\n",
      "loss: 0.573300  [51264/60000]\n",
      "loss: 0.422011  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.470267 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.313960  [   64/60000]\n",
      "loss: 0.467956  [ 6464/60000]\n",
      "loss: 0.290905  [12864/60000]\n",
      "loss: 0.498282  [19264/60000]\n",
      "loss: 0.412130  [25664/60000]\n",
      "loss: 0.436604  [32064/60000]\n",
      "loss: 0.448113  [38464/60000]\n",
      "loss: 0.638696  [44864/60000]\n",
      "loss: 0.571780  [51264/60000]\n",
      "loss: 0.420477  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.469064 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.311602  [   64/60000]\n",
      "loss: 0.466462  [ 6464/60000]\n",
      "loss: 0.289586  [12864/60000]\n",
      "loss: 0.496652  [19264/60000]\n",
      "loss: 0.410077  [25664/60000]\n",
      "loss: 0.434976  [32064/60000]\n",
      "loss: 0.446510  [38464/60000]\n",
      "loss: 0.637042  [44864/60000]\n",
      "loss: 0.570272  [51264/60000]\n",
      "loss: 0.419007  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.467885 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.309320  [   64/60000]\n",
      "loss: 0.464951  [ 6464/60000]\n",
      "loss: 0.288302  [12864/60000]\n",
      "loss: 0.495038  [19264/60000]\n",
      "loss: 0.408099  [25664/60000]\n",
      "loss: 0.433466  [32064/60000]\n",
      "loss: 0.444950  [38464/60000]\n",
      "loss: 0.635370  [44864/60000]\n",
      "loss: 0.568749  [51264/60000]\n",
      "loss: 0.417591  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.466728 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.307102  [   64/60000]\n",
      "loss: 0.463517  [ 6464/60000]\n",
      "loss: 0.287045  [12864/60000]\n",
      "loss: 0.493503  [19264/60000]\n",
      "loss: 0.406145  [25664/60000]\n",
      "loss: 0.431984  [32064/60000]\n",
      "loss: 0.443388  [38464/60000]\n",
      "loss: 0.633689  [44864/60000]\n",
      "loss: 0.567330  [51264/60000]\n",
      "loss: 0.416300  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.465599 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.304976  [   64/60000]\n",
      "loss: 0.462132  [ 6464/60000]\n",
      "loss: 0.285871  [12864/60000]\n",
      "loss: 0.492016  [19264/60000]\n",
      "loss: 0.404278  [25664/60000]\n",
      "loss: 0.430557  [32064/60000]\n",
      "loss: 0.441895  [38464/60000]\n",
      "loss: 0.631975  [44864/60000]\n",
      "loss: 0.565970  [51264/60000]\n",
      "loss: 0.415016  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.464494 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.302901  [   64/60000]\n",
      "loss: 0.460726  [ 6464/60000]\n",
      "loss: 0.284761  [12864/60000]\n",
      "loss: 0.490618  [19264/60000]\n",
      "loss: 0.402397  [25664/60000]\n",
      "loss: 0.429143  [32064/60000]\n",
      "loss: 0.440465  [38464/60000]\n",
      "loss: 0.630238  [44864/60000]\n",
      "loss: 0.564598  [51264/60000]\n",
      "loss: 0.413772  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.463409 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.300894  [   64/60000]\n",
      "loss: 0.459248  [ 6464/60000]\n",
      "loss: 0.283638  [12864/60000]\n",
      "loss: 0.489261  [19264/60000]\n",
      "loss: 0.400584  [25664/60000]\n",
      "loss: 0.427756  [32064/60000]\n",
      "loss: 0.439057  [38464/60000]\n",
      "loss: 0.628546  [44864/60000]\n",
      "loss: 0.563251  [51264/60000]\n",
      "loss: 0.412615  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.462347 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.298962  [   64/60000]\n",
      "loss: 0.457786  [ 6464/60000]\n",
      "loss: 0.282519  [12864/60000]\n",
      "loss: 0.487908  [19264/60000]\n",
      "loss: 0.398799  [25664/60000]\n",
      "loss: 0.426404  [32064/60000]\n",
      "loss: 0.437707  [38464/60000]\n",
      "loss: 0.626897  [44864/60000]\n",
      "loss: 0.561841  [51264/60000]\n",
      "loss: 0.411485  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.461303 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.297123  [   64/60000]\n",
      "loss: 0.456310  [ 6464/60000]\n",
      "loss: 0.281410  [12864/60000]\n",
      "loss: 0.486614  [19264/60000]\n",
      "loss: 0.396981  [25664/60000]\n",
      "loss: 0.425133  [32064/60000]\n",
      "loss: 0.436379  [38464/60000]\n",
      "loss: 0.625244  [44864/60000]\n",
      "loss: 0.560431  [51264/60000]\n",
      "loss: 0.410405  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.460278 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.295323  [   64/60000]\n",
      "loss: 0.454865  [ 6464/60000]\n",
      "loss: 0.280357  [12864/60000]\n",
      "loss: 0.485345  [19264/60000]\n",
      "loss: 0.395219  [25664/60000]\n",
      "loss: 0.423803  [32064/60000]\n",
      "loss: 0.434991  [38464/60000]\n",
      "loss: 0.623624  [44864/60000]\n",
      "loss: 0.558995  [51264/60000]\n",
      "loss: 0.409368  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.459269 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.293566  [   64/60000]\n",
      "loss: 0.453455  [ 6464/60000]\n",
      "loss: 0.279335  [12864/60000]\n",
      "loss: 0.484052  [19264/60000]\n",
      "loss: 0.393444  [25664/60000]\n",
      "loss: 0.422529  [32064/60000]\n",
      "loss: 0.433567  [38464/60000]\n",
      "loss: 0.621972  [44864/60000]\n",
      "loss: 0.557597  [51264/60000]\n",
      "loss: 0.408384  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.458285 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.291852  [   64/60000]\n",
      "loss: 0.452124  [ 6464/60000]\n",
      "loss: 0.278322  [12864/60000]\n",
      "loss: 0.482790  [19264/60000]\n",
      "loss: 0.391690  [25664/60000]\n",
      "loss: 0.421301  [32064/60000]\n",
      "loss: 0.432218  [38464/60000]\n",
      "loss: 0.620278  [44864/60000]\n",
      "loss: 0.556198  [51264/60000]\n",
      "loss: 0.407397  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.457316 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.290176  [   64/60000]\n",
      "loss: 0.450820  [ 6464/60000]\n",
      "loss: 0.277339  [12864/60000]\n",
      "loss: 0.481513  [19264/60000]\n",
      "loss: 0.389921  [25664/60000]\n",
      "loss: 0.420108  [32064/60000]\n",
      "loss: 0.430911  [38464/60000]\n",
      "loss: 0.618585  [44864/60000]\n",
      "loss: 0.554851  [51264/60000]\n",
      "loss: 0.406499  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.456360 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.288550  [   64/60000]\n",
      "loss: 0.449501  [ 6464/60000]\n",
      "loss: 0.276330  [12864/60000]\n",
      "loss: 0.480314  [19264/60000]\n",
      "loss: 0.388204  [25664/60000]\n",
      "loss: 0.418991  [32064/60000]\n",
      "loss: 0.429651  [38464/60000]\n",
      "loss: 0.616918  [44864/60000]\n",
      "loss: 0.553560  [51264/60000]\n",
      "loss: 0.405583  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.455420 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.286950  [   64/60000]\n",
      "loss: 0.448197  [ 6464/60000]\n",
      "loss: 0.275358  [12864/60000]\n",
      "loss: 0.479151  [19264/60000]\n",
      "loss: 0.386480  [25664/60000]\n",
      "loss: 0.417863  [32064/60000]\n",
      "loss: 0.428512  [38464/60000]\n",
      "loss: 0.615259  [44864/60000]\n",
      "loss: 0.552242  [51264/60000]\n",
      "loss: 0.404725  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.454491 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.285414  [   64/60000]\n",
      "loss: 0.446927  [ 6464/60000]\n",
      "loss: 0.274460  [12864/60000]\n",
      "loss: 0.478004  [19264/60000]\n",
      "loss: 0.384804  [25664/60000]\n",
      "loss: 0.416768  [32064/60000]\n",
      "loss: 0.427313  [38464/60000]\n",
      "loss: 0.613677  [44864/60000]\n",
      "loss: 0.550951  [51264/60000]\n",
      "loss: 0.403907  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.453574 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.283912  [   64/60000]\n",
      "loss: 0.445632  [ 6464/60000]\n",
      "loss: 0.273632  [12864/60000]\n",
      "loss: 0.476889  [19264/60000]\n",
      "loss: 0.383163  [25664/60000]\n",
      "loss: 0.415663  [32064/60000]\n",
      "loss: 0.426228  [38464/60000]\n",
      "loss: 0.612031  [44864/60000]\n",
      "loss: 0.549656  [51264/60000]\n",
      "loss: 0.403161  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.452677 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.282453  [   64/60000]\n",
      "loss: 0.444363  [ 6464/60000]\n",
      "loss: 0.272721  [12864/60000]\n",
      "loss: 0.475699  [19264/60000]\n",
      "loss: 0.381603  [25664/60000]\n",
      "loss: 0.414565  [32064/60000]\n",
      "loss: 0.425070  [38464/60000]\n",
      "loss: 0.610383  [44864/60000]\n",
      "loss: 0.548421  [51264/60000]\n",
      "loss: 0.402399  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.451785 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.280972  [   64/60000]\n",
      "loss: 0.443100  [ 6464/60000]\n",
      "loss: 0.271847  [12864/60000]\n",
      "loss: 0.474580  [19264/60000]\n",
      "loss: 0.380084  [25664/60000]\n",
      "loss: 0.413470  [32064/60000]\n",
      "loss: 0.423842  [38464/60000]\n",
      "loss: 0.608851  [44864/60000]\n",
      "loss: 0.547186  [51264/60000]\n",
      "loss: 0.401690  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.450911 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.279561  [   64/60000]\n",
      "loss: 0.441891  [ 6464/60000]\n",
      "loss: 0.270974  [12864/60000]\n",
      "loss: 0.473434  [19264/60000]\n",
      "loss: 0.378561  [25664/60000]\n",
      "loss: 0.412414  [32064/60000]\n",
      "loss: 0.422608  [38464/60000]\n",
      "loss: 0.607322  [44864/60000]\n",
      "loss: 0.545926  [51264/60000]\n",
      "loss: 0.401007  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.450050 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.278196  [   64/60000]\n",
      "loss: 0.440663  [ 6464/60000]\n",
      "loss: 0.270143  [12864/60000]\n",
      "loss: 0.472313  [19264/60000]\n",
      "loss: 0.377087  [25664/60000]\n",
      "loss: 0.411347  [32064/60000]\n",
      "loss: 0.421333  [38464/60000]\n",
      "loss: 0.605911  [44864/60000]\n",
      "loss: 0.544731  [51264/60000]\n",
      "loss: 0.400301  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.449199 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.276866  [   64/60000]\n",
      "loss: 0.439498  [ 6464/60000]\n",
      "loss: 0.269289  [12864/60000]\n",
      "loss: 0.471198  [19264/60000]\n",
      "loss: 0.375571  [25664/60000]\n",
      "loss: 0.410292  [32064/60000]\n",
      "loss: 0.420095  [38464/60000]\n",
      "loss: 0.604478  [44864/60000]\n",
      "loss: 0.543471  [51264/60000]\n",
      "loss: 0.399585  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.1%, Avg loss: 0.448349 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.275580  [   64/60000]\n",
      "loss: 0.438318  [ 6464/60000]\n",
      "loss: 0.268460  [12864/60000]\n",
      "loss: 0.470111  [19264/60000]\n",
      "loss: 0.374116  [25664/60000]\n",
      "loss: 0.409201  [32064/60000]\n",
      "loss: 0.418921  [38464/60000]\n",
      "loss: 0.603033  [44864/60000]\n",
      "loss: 0.542278  [51264/60000]\n",
      "loss: 0.398867  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.1%, Avg loss: 0.447512 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.274318  [   64/60000]\n",
      "loss: 0.437166  [ 6464/60000]\n",
      "loss: 0.267652  [12864/60000]\n",
      "loss: 0.469055  [19264/60000]\n",
      "loss: 0.372687  [25664/60000]\n",
      "loss: 0.408063  [32064/60000]\n",
      "loss: 0.417761  [38464/60000]\n",
      "loss: 0.601609  [44864/60000]\n",
      "loss: 0.541123  [51264/60000]\n",
      "loss: 0.398081  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.1%, Avg loss: 0.446681 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.273097  [   64/60000]\n",
      "loss: 0.435994  [ 6464/60000]\n",
      "loss: 0.266895  [12864/60000]\n",
      "loss: 0.467949  [19264/60000]\n",
      "loss: 0.371304  [25664/60000]\n",
      "loss: 0.406951  [32064/60000]\n",
      "loss: 0.416617  [38464/60000]\n",
      "loss: 0.600232  [44864/60000]\n",
      "loss: 0.539946  [51264/60000]\n",
      "loss: 0.397312  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.1%, Avg loss: 0.445854 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.271882  [   64/60000]\n",
      "loss: 0.434874  [ 6464/60000]\n",
      "loss: 0.266163  [12864/60000]\n",
      "loss: 0.466902  [19264/60000]\n",
      "loss: 0.369909  [25664/60000]\n",
      "loss: 0.405895  [32064/60000]\n",
      "loss: 0.415525  [38464/60000]\n",
      "loss: 0.598807  [44864/60000]\n",
      "loss: 0.538774  [51264/60000]\n",
      "loss: 0.396572  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.445033 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.270715  [   64/60000]\n",
      "loss: 0.433740  [ 6464/60000]\n",
      "loss: 0.265434  [12864/60000]\n",
      "loss: 0.465776  [19264/60000]\n",
      "loss: 0.368515  [25664/60000]\n",
      "loss: 0.404906  [32064/60000]\n",
      "loss: 0.414440  [38464/60000]\n",
      "loss: 0.597469  [44864/60000]\n",
      "loss: 0.537606  [51264/60000]\n",
      "loss: 0.395935  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.444215 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.269552  [   64/60000]\n",
      "loss: 0.432602  [ 6464/60000]\n",
      "loss: 0.264668  [12864/60000]\n",
      "loss: 0.464734  [19264/60000]\n",
      "loss: 0.367194  [25664/60000]\n",
      "loss: 0.403940  [32064/60000]\n",
      "loss: 0.413328  [38464/60000]\n",
      "loss: 0.596179  [44864/60000]\n",
      "loss: 0.536257  [51264/60000]\n",
      "loss: 0.395319  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.443396 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.268483  [   64/60000]\n",
      "loss: 0.431446  [ 6464/60000]\n",
      "loss: 0.263966  [12864/60000]\n",
      "loss: 0.463699  [19264/60000]\n",
      "loss: 0.365804  [25664/60000]\n",
      "loss: 0.402738  [32064/60000]\n",
      "loss: 0.412161  [38464/60000]\n",
      "loss: 0.594931  [44864/60000]\n",
      "loss: 0.534901  [51264/60000]\n",
      "loss: 0.394621  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.442573 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.267423  [   64/60000]\n",
      "loss: 0.430291  [ 6464/60000]\n",
      "loss: 0.263326  [12864/60000]\n",
      "loss: 0.462649  [19264/60000]\n",
      "loss: 0.364349  [25664/60000]\n",
      "loss: 0.401425  [32064/60000]\n",
      "loss: 0.411024  [38464/60000]\n",
      "loss: 0.593785  [44864/60000]\n",
      "loss: 0.533697  [51264/60000]\n",
      "loss: 0.393923  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.441748 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.266305  [   64/60000]\n",
      "loss: 0.429159  [ 6464/60000]\n",
      "loss: 0.262666  [12864/60000]\n",
      "loss: 0.461690  [19264/60000]\n",
      "loss: 0.362974  [25664/60000]\n",
      "loss: 0.400234  [32064/60000]\n",
      "loss: 0.409817  [38464/60000]\n",
      "loss: 0.592403  [44864/60000]\n",
      "loss: 0.532430  [51264/60000]\n",
      "loss: 0.393024  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.440887 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.265276  [   64/60000]\n",
      "loss: 0.427947  [ 6464/60000]\n",
      "loss: 0.261938  [12864/60000]\n",
      "loss: 0.460669  [19264/60000]\n",
      "loss: 0.361580  [25664/60000]\n",
      "loss: 0.399250  [32064/60000]\n",
      "loss: 0.408765  [38464/60000]\n",
      "loss: 0.591108  [44864/60000]\n",
      "loss: 0.531473  [51264/60000]\n",
      "loss: 0.392358  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.440068 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.264250  [   64/60000]\n",
      "loss: 0.426682  [ 6464/60000]\n",
      "loss: 0.261146  [12864/60000]\n",
      "loss: 0.459729  [19264/60000]\n",
      "loss: 0.360155  [25664/60000]\n",
      "loss: 0.398114  [32064/60000]\n",
      "loss: 0.407636  [38464/60000]\n",
      "loss: 0.589660  [44864/60000]\n",
      "loss: 0.530413  [51264/60000]\n",
      "loss: 0.391831  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.439283 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.263272  [   64/60000]\n",
      "loss: 0.425392  [ 6464/60000]\n",
      "loss: 0.260416  [12864/60000]\n",
      "loss: 0.458822  [19264/60000]\n",
      "loss: 0.358779  [25664/60000]\n",
      "loss: 0.396900  [32064/60000]\n",
      "loss: 0.406208  [38464/60000]\n",
      "loss: 0.588203  [44864/60000]\n",
      "loss: 0.529364  [51264/60000]\n",
      "loss: 0.391283  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.438515 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.262296  [   64/60000]\n",
      "loss: 0.424181  [ 6464/60000]\n",
      "loss: 0.259663  [12864/60000]\n",
      "loss: 0.457751  [19264/60000]\n",
      "loss: 0.357415  [25664/60000]\n",
      "loss: 0.395769  [32064/60000]\n",
      "loss: 0.404862  [38464/60000]\n",
      "loss: 0.587004  [44864/60000]\n",
      "loss: 0.528292  [51264/60000]\n",
      "loss: 0.390649  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.437768 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.261363  [   64/60000]\n",
      "loss: 0.423041  [ 6464/60000]\n",
      "loss: 0.258935  [12864/60000]\n",
      "loss: 0.456690  [19264/60000]\n",
      "loss: 0.356151  [25664/60000]\n",
      "loss: 0.394716  [32064/60000]\n",
      "loss: 0.403665  [38464/60000]\n",
      "loss: 0.585857  [44864/60000]\n",
      "loss: 0.527149  [51264/60000]\n",
      "loss: 0.390065  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.437033 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.260471  [   64/60000]\n",
      "loss: 0.421891  [ 6464/60000]\n",
      "loss: 0.258230  [12864/60000]\n",
      "loss: 0.455597  [19264/60000]\n",
      "loss: 0.354874  [25664/60000]\n",
      "loss: 0.393673  [32064/60000]\n",
      "loss: 0.402454  [38464/60000]\n",
      "loss: 0.584727  [44864/60000]\n",
      "loss: 0.526002  [51264/60000]\n",
      "loss: 0.389478  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.436304 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_data_analysis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
