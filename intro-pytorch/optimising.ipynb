{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base model and data that we're going to train\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set hyperparameters first (parameters that arent trained)\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we're going to train this model in an optimisation loop. each iteration of the loop is called an epoch\n",
    "\n",
    "every epoch got two parts:\n",
    "- train loop - iterate over the train dataset and attempt to converge to optimal parameters\n",
    "- validation/test loop - iterate over test dataset and see if performance is improving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train loop:\n",
    "\n",
    "the loss function is very important here. it measures the level of dissimilarity of the predicted to the actual result. \n",
    "to calculate loss, we make a prediction and compare it to the true data label value (ground truth)\n",
    "\n",
    "Common loss functions include nn.MSELoss (Mean Square Error) for regression tasks, and nn.NLLLoss (Negative Log Likelihood) for classification. nn.CrossEntropyLoss combines nn.LogSoftmax and nn.NLLLoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss() # normalize the logits and compute the prediction error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimisation is adjusting model params to reduce the model error. optimisation algos determine how this is performed. \n",
    "\n",
    "for this example, we use stochastic gradient descent. all optimisation logic is encapsulated and abstracted within the optimizer object. \n",
    "\n",
    "other optimisers include ADAM or RMSProp, which work better for different models and data. \n",
    "\n",
    "We initialize the optimizer by registering the modelâ€™s parameters that need to be trained, and passing in the learning rate hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the training loop, optimization happens in three steps:\n",
    "1. Call optimizer.zero_grad() to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
    "\n",
    "2. Backpropagate the prediction loss with a call to loss.backward(). PyTorch deposits the gradients of the loss w.r.t. each parameter.\n",
    "\n",
    "3. Once we have our gradients, we call optimizer.step() to adjust the parameters by the gradients collected in the backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>full implementation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward() # compute the gradient of the loss with respect to the model's parameters\n",
    "        optimizer.step() # adjust the model's parameters based on the computed gradients\n",
    "        optimizer.zero_grad() # reset the gradients to zero\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.306755  [   64/60000]\n",
      "loss: 2.298370  [ 6464/60000]\n",
      "loss: 2.274577  [12864/60000]\n",
      "loss: 2.266144  [19264/60000]\n",
      "loss: 2.257879  [25664/60000]\n",
      "loss: 2.219342  [32064/60000]\n",
      "loss: 2.227928  [38464/60000]\n",
      "loss: 2.198219  [44864/60000]\n",
      "loss: 2.202961  [51264/60000]\n",
      "loss: 2.153521  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.2%, Avg loss: 2.162516 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.176166  [   64/60000]\n",
      "loss: 2.169892  [ 6464/60000]\n",
      "loss: 2.112028  [12864/60000]\n",
      "loss: 2.122149  [19264/60000]\n",
      "loss: 2.080901  [25664/60000]\n",
      "loss: 2.011679  [32064/60000]\n",
      "loss: 2.039401  [38464/60000]\n",
      "loss: 1.970496  [44864/60000]\n",
      "loss: 1.985421  [51264/60000]\n",
      "loss: 1.886009  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 57.1%, Avg loss: 1.905084 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.943356  [   64/60000]\n",
      "loss: 1.914415  [ 6464/60000]\n",
      "loss: 1.798731  [12864/60000]\n",
      "loss: 1.831680  [19264/60000]\n",
      "loss: 1.723822  [25664/60000]\n",
      "loss: 1.666452  [32064/60000]\n",
      "loss: 1.686455  [38464/60000]\n",
      "loss: 1.595946  [44864/60000]\n",
      "loss: 1.624650  [51264/60000]\n",
      "loss: 1.493487  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.530266 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.599622  [   64/60000]\n",
      "loss: 1.564003  [ 6464/60000]\n",
      "loss: 1.414263  [12864/60000]\n",
      "loss: 1.481330  [19264/60000]\n",
      "loss: 1.352292  [25664/60000]\n",
      "loss: 1.350264  [32064/60000]\n",
      "loss: 1.358257  [38464/60000]\n",
      "loss: 1.287805  [44864/60000]\n",
      "loss: 1.322807  [51264/60000]\n",
      "loss: 1.210390  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 1.246995 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.325254  [   64/60000]\n",
      "loss: 1.308568  [ 6464/60000]\n",
      "loss: 1.138457  [12864/60000]\n",
      "loss: 1.249501  [19264/60000]\n",
      "loss: 1.114149  [25664/60000]\n",
      "loss: 1.142104  [32064/60000]\n",
      "loss: 1.160859  [38464/60000]\n",
      "loss: 1.098091  [44864/60000]\n",
      "loss: 1.138590  [51264/60000]\n",
      "loss: 1.049506  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.076995 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.147652  [   64/60000]\n",
      "loss: 1.151814  [ 6464/60000]\n",
      "loss: 0.962813  [12864/60000]\n",
      "loss: 1.110500  [19264/60000]\n",
      "loss: 0.976618  [25664/60000]\n",
      "loss: 1.007281  [32064/60000]\n",
      "loss: 1.043149  [38464/60000]\n",
      "loss: 0.981981  [44864/60000]\n",
      "loss: 1.025423  [51264/60000]\n",
      "loss: 0.952485  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.971569 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.029949  [   64/60000]\n",
      "loss: 1.053490  [ 6464/60000]\n",
      "loss: 0.846828  [12864/60000]\n",
      "loss: 1.019737  [19264/60000]\n",
      "loss: 0.893297  [25664/60000]\n",
      "loss: 0.914506  [32064/60000]\n",
      "loss: 0.967380  [38464/60000]\n",
      "loss: 0.908288  [44864/60000]\n",
      "loss: 0.950155  [51264/60000]\n",
      "loss: 0.888928  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.901636 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.946180  [   64/60000]\n",
      "loss: 0.986673  [ 6464/60000]\n",
      "loss: 0.765964  [12864/60000]\n",
      "loss: 0.956416  [19264/60000]\n",
      "loss: 0.839288  [25664/60000]\n",
      "loss: 0.848149  [32064/60000]\n",
      "loss: 0.914670  [38464/60000]\n",
      "loss: 0.859877  [44864/60000]\n",
      "loss: 0.897505  [51264/60000]\n",
      "loss: 0.843939  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.852169 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.882585  [   64/60000]\n",
      "loss: 0.937425  [ 6464/60000]\n",
      "loss: 0.706583  [12864/60000]\n",
      "loss: 0.909548  [19264/60000]\n",
      "loss: 0.801089  [25664/60000]\n",
      "loss: 0.798748  [32064/60000]\n",
      "loss: 0.874568  [38464/60000]\n",
      "loss: 0.826143  [44864/60000]\n",
      "loss: 0.858632  [51264/60000]\n",
      "loss: 0.809481  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.814761 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.831486  [   64/60000]\n",
      "loss: 0.898154  [ 6464/60000]\n",
      "loss: 0.660653  [12864/60000]\n",
      "loss: 0.873277  [19264/60000]\n",
      "loss: 0.771790  [25664/60000]\n",
      "loss: 0.760662  [32064/60000]\n",
      "loss: 0.841710  [38464/60000]\n",
      "loss: 0.801009  [44864/60000]\n",
      "loss: 0.828316  [51264/60000]\n",
      "loss: 0.781777  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.784764 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gesture_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
