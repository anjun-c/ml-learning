{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self): #initialise nn layers\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512), #a linear layer with 28*28 (784) input features, 512 output features\n",
    "            #it has 512 neurons, which all have a weight (w) for each of the 784 input features (x)\n",
    "            #every neuron also has a bias (b)\n",
    "            #i.e. neuron = sum(W*x + b)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x): #occurs when data is input into nn\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights and biases of each neuron are trained to be different through the process of backpropagation and optimization during the training of the neural network:\n",
    "\n",
    "Initialization: Initially, the weights and biases are typically initialized randomly or using specific initialization techniques.\n",
    "\n",
    "Forward Pass: During the forward pass, the input data is passed through the network, and each neuron computes its output using its current weights and biases.\n",
    "\n",
    "Loss Calculation: The network's output is compared to the true labels using a loss function, which quantifies the difference between the predicted and actual values.\n",
    "\n",
    "Backpropagation: The loss is propagated backward through the network. During this process, the gradients of the loss with respect to each weight and bias are computed using the chain rule of calculus.\n",
    "\n",
    "Optimization: An optimization algorithm (e.g., Stochastic Gradient Descent, Adam) updates the weights and biases using the computed gradients. The update rule typically looks like this: [ \\text{weight} = \\text{weight} - \\text{learning rate} \\times \\text{gradient of weight} ] [ \\text{bias} = \\text{bias} - \\text{learning rate} \\times \\text{gradient of bias} ]\n",
    "\n",
    "Iteration: Steps 2-5 are repeated for many epochs (iterations over the entire dataset), gradually adjusting the weights and biases to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0296, -0.0298,  0.0475,  0.1090,  0.0137, -0.0965, -0.1441,  0.1119,\n",
       "          0.0360,  0.1035]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X) #10 raw predicted values for each class, this also runs forward once\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1008, 0.0950, 0.1026, 0.1092, 0.0992, 0.0889, 0.0847, 0.1095, 0.1015,\n",
       "         0.1086]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_probab = nn.Softmax(dim=1)(logits) #finds the prediction probabilities\n",
    "pred_probab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([7])\n"
     ]
    }
   ],
   "source": [
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\") #highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "#model layer demonstration\n",
    "#sample mini batch\n",
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "#flatten converts the 2D 28x28 image into an array of 784 pixel values\n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "#applies a linear transformation on the input based on stored weights and biases\n",
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.2581, -0.1897, -0.2964,  0.6508, -0.1967,  0.4134,  0.2895,  0.1934,\n",
      "          0.6775,  0.3864, -0.0812,  0.2852, -0.0442, -0.0150, -0.0426, -0.1924,\n",
      "         -0.5672, -0.4487, -0.0916,  0.2519],\n",
      "        [ 0.4474, -0.0427,  0.0074,  0.8741,  0.0471, -0.0846,  0.2101,  0.1953,\n",
      "          0.2255,  0.1774,  0.0940,  0.2141,  0.0608, -0.1953,  0.6072, -0.4226,\n",
      "         -0.1450, -0.3199,  0.1953, -0.3686],\n",
      "        [ 0.3435, -0.0528, -0.1857,  0.5968, -0.0433,  0.1711,  0.0752,  0.3432,\n",
      "          0.2639,  0.0344,  0.0628,  0.4501,  0.1666, -0.2169,  0.5048,  0.0233,\n",
      "         -0.1420, -0.6633, -0.0027, -0.1372]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.2581, 0.0000, 0.0000, 0.6508, 0.0000, 0.4134, 0.2895, 0.1934, 0.6775,\n",
      "         0.3864, 0.0000, 0.2852, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2519],\n",
      "        [0.4474, 0.0000, 0.0074, 0.8741, 0.0471, 0.0000, 0.2101, 0.1953, 0.2255,\n",
      "         0.1774, 0.0940, 0.2141, 0.0608, 0.0000, 0.6072, 0.0000, 0.0000, 0.0000,\n",
      "         0.1953, 0.0000],\n",
      "        [0.3435, 0.0000, 0.0000, 0.5968, 0.0000, 0.1711, 0.0752, 0.3432, 0.2639,\n",
      "         0.0344, 0.0628, 0.4501, 0.1666, 0.0000, 0.5048, 0.0233, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#ReLU activation function \n",
    "#determines whether neurons are active or not, transforms data and adds non linearity to nns\n",
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")\n",
    "#but a problem with ReLU is that it can cause alot of dead neurons, because negative values get turned into zero\n",
    "#below you can see a decent amount of dead neurons - not catastrophic but could cause problems if we decide to make the network deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn.Sequential is an ordered container of modules - e.g. data is passed through the modules in the order that u wrote\n",
    "# u can use this to speed make a nn\n",
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the last layer of the nn returns logits [-inf,inf] range\n",
    "\n",
    "we pass this onto the softmax fn which scales it to values range [0,1]\n",
    "\n",
    "represents predicted probability of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dim parameter is indicating the dimension which values must sum to 1\n",
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most layers in an nn are parameterised i.e. have weights and biases (which are then optimised in training)\n",
    "subclassing nn.Module tracks all these fields, so you can see the parameters using parameters() or named_parameters()\n",
    "\n",
    "this example iterates through every parameter, prints size then preview of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0185,  0.0333, -0.0154,  ..., -0.0127,  0.0203, -0.0217],\n",
      "        [-0.0047,  0.0294, -0.0304,  ..., -0.0239, -0.0148, -0.0238]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([0.0161, 0.0330], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0385, -0.0351,  0.0363,  ...,  0.0411, -0.0147,  0.0365],\n",
      "        [-0.0293,  0.0306, -0.0183,  ...,  0.0421, -0.0337, -0.0442]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([0.0384, 0.0376], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0439,  0.0437,  0.0133,  ...,  0.0046,  0.0064, -0.0355],\n",
      "        [ 0.0349, -0.0179, -0.0342,  ..., -0.0142,  0.0213,  0.0031]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0038, -0.0081], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gesture_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
